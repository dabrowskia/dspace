---
title: "Introduction to the dspace package"
author: Adam DÄ…browski, Jakub Nowosad
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{introduction-to-the-dspace-package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(dspace)
```

Dspace (divide space) is designed for spatial segmentation/clustering/regionalization of **point or polygon data of class 'sf'**. It uses igraph to build a topological network/graph connecting points or polygons centroids. The nodes of the graph are connected with their nearest neighbours based on defined criteria (eg. nearest neighbours or queen/rook neighbourhood - see...) and the wieght of the edge is calculated as similarity between two nodes. To find network communities which represent regions/spatial clusters/submaretes it uses fast greedy algorithm (add citation) .

This package has been developed mainly for **real estate market segmentation (submarket delimitation)** but can be easily adapted to any variety of spatial data.

The package uses following functions:

 * regionalize - which devides data regardles of geometry type (point/polygon) into spatially coheren regions,
 * points_ds - which devides point data into spatially coherent regions, #function deprecated
 * polygons_ds - which devides polygon data into spatially coherent regions, #function deprecated
 * find_no_clusters - which analyzes a range of divisions (by default from 2 to 30 regions) to find the one with highest modularity value (add citation)
 * plot_modularity - which visualizes the result of find_no_clusters to find the best number of clusters to divide your data

# Where to start

```
data(realEstate)
realEstate.modularity <- regionalize(realEstate, polygon = FALSE)
plot_modularity(realEstate.modularity)
realEstate$class <- points_ds(realEstate, k = 5, accuracy = FALSE)
```

# Overview of the algorithm
Dspace works with point or polygon datasets to regionalize - divide space into spatially coherent regions/clusters. The mechanism behind the algorithm uses network communities for data fixed in geographic 2-deimnsional space. If the input dataset are points, then based on their position an graph network is beeing build that can connect points based on their number of nearest neighbours (in future development other means of building network will be implemented). If polygons are the input data then graph can be built based on the queen/rook neighbourhood of the polygons.

When a graph connecting all of the neighbouring points/polygons is created  a similarity metric is calculated between the neighbouring objects. This similarity can be calulated based on different distance metrics: "mahalanobis" or "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowisk". If method is one of "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski", see dist for details, because this function is used to compute the distance. If method is set to "mahalanobis", the mahalanobis distance is computed between neighbour points. If method is a function, this function is used to compute the distance.

Based on  a graph with weights assigned to the edges between the objects (points and polygons) a fast greedy algorithm is used to determine spatial clusters (communities in a graph).
 
# Finding number of clusters
To find appropriate number of clusters a `find_no_clusters` function has been created. It creates a rapid division of data into a defined number of regions (by default from 2 to 30 regions) and caluclates for each division into communities their modularity metric (**needs reference for modularity**). By default the function returns a named vector which can be applied to the function `plot_modularity` that uses ggplot to visulaize changes in modularity for each division.

# Assessing the division into clusters
For assessing the goodness of division while using `regionalize` function (or `point_ds`/`polygon_ds`) an argument `accuracy` can be set to `TRUE`. While it is set to `TRUE`, a randomForest model is being trained to establish how much the division of the regions could be implied based on the variables from the dataset itself without taking into account their position in geographical space. **this absolutly needs refinement**
